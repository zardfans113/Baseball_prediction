import pandas as pd
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.font_manager as fm

# è¨­å®šæ—¥æ–‡å­—é«”ï¼ˆä½¿ç”¨ MS Gothicã€Yu Gothicã€Meiryo æˆ– Noto Sans CJK JPï¼‰
plt.rcParams['font.sans-serif'] = ['MS Gothic']  # æˆ– 'Meiryo', 'Yu Gothic', 'Noto Sans CJK JP'
plt.rcParams['axes.unicode_minus'] = False  # é¿å…è² è™Ÿé¡¯ç¤ºéŒ¯èª¤

# ============ ğŸ“Œ Step 1: Read and Process Data ============
file_paths = {
    "2021": "2021_qualified_hitters.csv",
    "2022": "2022_qualified_hitters.csv",
    "2023": "2023_qualified_hitters.csv",
    "2024": "2024_qualified_hitters.csv"
}

dfs = []
for year, path in file_paths.items():
    df = pd.read_csv(path)
    df["year"] = int(year)  # Add year column
    dfs.append(df)

df_all = pd.concat(dfs, ignore_index=True)

# Clean data: Remove unnecessary characters in player names
df_all["é¸æ‰‹å"] = df_all["é¸æ‰‹å"].str.split(":").str[-1]

# Convert columns to correct data types
float_cols = ["æ‰“ ç‡", "å‡º å¡ ç‡", "é•· æ‰“ ç‡", "O P S"]
int_cols = ["æœ¬ å¡ æ‰“", "æ‰“ ç‚¹"]

for col in float_cols:
    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')
for col in int_cols:
    df_all[col] = pd.to_numeric(df_all[col], errors='coerce').fillna(0).astype(int)

# Select features and target variables
features = ["æ‰“ å¸­ æ•°", "æ‰“ ç‡", "æœ¬ å¡ æ‰“", "å‡º å¡ ç‡", "O P S"]
targets = ["æ‰“ ç‡", "æœ¬ å¡ æ‰“", "æ‰“ ç‚¹", "å‡º å¡ ç‡", "é•· æ‰“ ç‡", "O P S"]

df_cleaned = df_all.dropna(subset=features + targets)

# ============ ğŸ“Œ Step 2: Split Train/Test Data ============
train_df = df_cleaned[df_cleaned["year"] < 2024]  # Train on 2021-2023
test_df = df_cleaned[df_cleaned["year"] == 2024]  # Test on 2024

X_train = train_df[features]
y_train = train_df[targets]
X_test = test_df[features]
y_test = test_df[targets]

# ============ ğŸ“Œ Step 3: Experiment - Effect of Training Data Size ============
train_sizes = [0.5, 0.75, 1.0]  # 50%, 75%, 100% training data
results = {}

for size in train_sizes:
    print(f"\nğŸ”¹ Training Data Size: {size * 100:.0f}%")
    
    if size == 1.0:
        # Use full training data without splitting
        X_train_partial = X_train
        y_train_partial = y_train
    else:
        # Split only if size < 100%
        X_train_partial, _, y_train_partial, _ = train_test_split(
            X_train, y_train, train_size=size, random_state=42
        )
    
    # Train XGBoost
    xgb_model = xgb.XGBRegressor(
        objective='reg:squarederror', n_estimators=10000, learning_rate=0.001, 
        max_depth=8, subsample=0.9, colsample_bytree=0.7
    )
    xgb_model.fit(X_train_partial, y_train_partial)
    y_pred_xgb = xgb_model.predict(X_test)
    
    # Train RandomForest
    rf_model = RandomForestRegressor(n_estimators=250, max_depth=10, random_state=42)
    rf_model.fit(X_train_partial, y_train_partial)
    y_pred_rf = rf_model.predict(X_test)
    
    # Calculate performance metrics
    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
    mse_xgb = mean_squared_error(y_test, y_pred_xgb)
    r2_xgb = r2_score(y_test, y_pred_xgb)
    
    mae_rf = mean_absolute_error(y_test, y_pred_rf)
    mse_rf = mean_squared_error(y_test, y_pred_rf)
    r2_rf = r2_score(y_test, y_pred_rf)

    results[size] = {
        "XGBoost MAE": mae_xgb, "RandomForest MAE": mae_rf,
        "XGBoost MSE": mse_xgb, "RandomForest MSE": mse_rf,
        "XGBoost RÂ²": r2_xgb, "RandomForest RÂ²": r2_rf
    }

    print(f"  XGBoost â†’ MAE: {mae_xgb:.4f}, MSE: {mse_xgb:.4f}, RÂ²: {r2_xgb:.4f}")
    print(f"  RandomForest â†’ MAE: {mae_rf:.4f}, MSE: {mse_rf:.4f}, RÂ²: {r2_rf:.4f}")

# ============ ğŸ“Œ Step 4: Visualizing Results ============
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

metrics = ["MAE", "MSE", "RÂ²"]
for i, metric in enumerate(metrics):
    axes[i].plot(train_sizes, [results[size][f"XGBoost {metric}"] for size in train_sizes], label="XGBoost", marker="o", linestyle='dashed')
    axes[i].plot(train_sizes, [results[size][f"RandomForest {metric}"] for size in train_sizes], label="RandomForest", marker="o")
    axes[i].set_xlabel("Training Data Size")
    axes[i].set_ylabel(metric)
    axes[i].set_title(f"Effect of Training Data Size on {metric}")
    axes[i].legend()

plt.tight_layout()
plt.show()